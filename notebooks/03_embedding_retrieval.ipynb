{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0674540",
   "metadata": {},
   "source": [
    "# Embedding Generation and Semantic Retrieval\n",
    "\n",
    "This notebook implements the **embedding and retrieval stage** of the RAG pipeline.\n",
    "Chunked transcript data is converted into dense vector representations and indexed\n",
    "using a vector database to enable semantic similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a196ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e28173",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/chunks\"\n",
    "VIDEO_ID = \"Gfr50f6ZBvo\"\n",
    "\n",
    "file_path = os.path.join(DATA_PATH, f\"{VIDEO_ID}_chunks.txt\")\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Chunk file loaded.\")\n",
    "print(f\"Total characters: {len(raw_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chunks(text: str) -> List[Document]:\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith(\"--- Chunk\"):\n",
    "            if current_chunk:\n",
    "                chunks.append(Document(page_content=\" \".join(current_chunk)))\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            if line.strip():\n",
    "                current_chunk.append(line.strip())\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(Document(page_content=\" \".join(current_chunk)))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "documents = parse_chunks(raw_text)\n",
    "\n",
    "print(f\"Total chunks loaded: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"Embedding model initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39832d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"Vector store created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic discussed in this video?\"\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=4)\n",
    "\n",
    "print(\"Top retrieved chunks:\\n\")\n",
    "\n",
    "for i, doc in enumerate(results, start=1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    print(doc.page_content[:400])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_scores = vectorstore.similarity_search_with_score(query, k=4)\n",
    "\n",
    "for i, (doc, score) in enumerate(results_with_scores, start=1):\n",
    "    print(f\"Result {i} | Distance Score: {score:.4f}\")\n",
    "    print(doc.page_content[:300])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DB_PATH = \"../vectorstore/faiss_index\"\n",
    "\n",
    "os.makedirs(VECTOR_DB_PATH, exist_ok=True)\n",
    "\n",
    "vectorstore.save_local(VECTOR_DB_PATH)\n",
    "\n",
    "print(f\"Vector store saved to: {VECTOR_DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vectorstore = FAISS.load_local(\n",
    "    VECTOR_DB_PATH,\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "test_results = loaded_vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(\"Vector store successfully reloaded.\")\n",
    "print(test_results[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011fb19",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Dense embeddings capture semantic similarity effectively for long-form transcripts.\n",
    "- FAISS provides fast and reliable nearest-neighbor retrieval.\n",
    "- Retrieved chunks are contextually aligned with the query, validating the chunking strategy.\n",
    "\n",
    "The vector store is now ready to be integrated with an LLM for answer generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dadb332",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Generated dense embeddings for transcript chunks\n",
    "- Indexed chunks using FAISS vector database\n",
    "- Validated semantic retrieval through similarity search\n",
    "- Persisted vector store for reuse\n",
    "\n",
    "**Next step:** Retrieval-Augmented Generation using an LLM  \n",
    "(`04_rag_pipeline.ipynb`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ca0fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting NeuralTranscript Indexing Pipeline ---\n",
      "üì• Successfully loaded 169 chunks from disk.\n",
      "üß† Initializing Neural Embedding Model (HuggingFace)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AgenticRAG\\LangChain-Transcript-QA\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Generating embeddings for 169 chunks. Please wait...\n",
      "üíæ FAISS Index successfully saved to: data/faiss_index\n",
      "\n",
      "üîç SIMILARITY SEARCH TEST:\n",
      "\n",
      "Result 1 (Source: Gfr50f6ZBvo):\n",
      "from a sentient animal and we know they're made of the same things biological neurons so we're gonna have to come up with explanations uh or models of the gap between substrate differences between mac...\n",
      "\n",
      "Result 2 (Source: Gfr50f6ZBvo):\n",
      "part of of birthing ai and that being the greatest benefit to humanity of any tool or technology ever and and getting us into a world of radical abundance and curing diseases and and and solving many ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: NeuralTranscript: Semantic Search & Q&A for YouTube Content\n",
    "MODULE: 03_VECTOR_INDEXING\n",
    "-------------------------------------------------------------------------\n",
    "DESCRIPTION:\n",
    "This module converts text chunks into high-dimensional vector embeddings \n",
    "and indexes them using FAISS (Facebook AI Similarity Search). This enables \n",
    "the system to retrieve context based on semantic similarity.\n",
    "\n",
    "AUTHOR: Engr. Inam Ullah Khan\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# Note: Use your chunked_docs from the previous step. \n",
    "# In a real pipeline, you might reload them or run them in the same session.\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "INDEX_SAVE_PATH = \"data/faiss_index\"\n",
    "\n",
    "# --- 2. CORE FUNCTIONS ---\n",
    "\n",
    "def generate_vector_store(documents):\n",
    "    \"\"\"\n",
    "    Converts documents to embeddings and stores them in a FAISS index.\n",
    "    \"\"\"\n",
    "    print(\"üß† Initializing Neural Embedding Model (HuggingFace)...\")\n",
    "    \n",
    "    # Using a high-quality, lightweight model included in your requirements\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'} # Use 'cuda' if you have a GPU in Colab\n",
    "    )\n",
    "    \n",
    "    print(f\"üöÄ Generating embeddings for {len(documents)} chunks. Please wait...\")\n",
    "    \n",
    "    # Create the FAISS index from the documents\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def save_index(vector_store, path):\n",
    "    \"\"\"Persists the FAISS index to the local disk.\"\"\"\n",
    "    vector_store.save_local(path)\n",
    "    print(f\"üíæ FAISS Index successfully saved to: {path}\")\n",
    "\n",
    "# --- 3. EXECUTION PIPELINE (UPDATED) ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting NeuralTranscript Indexing Pipeline ---\")\n",
    "    \n",
    "    # NEW: Load the chunks from the disk\n",
    "    try:\n",
    "        with open(\"data/chunked_docs.pkl\", \"rb\") as f:\n",
    "            chunked_docs = pickle.load(f)\n",
    "        print(f\"üì• Successfully loaded {len(chunked_docs)} chunks from disk.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: chunked_docs.pkl not found. Please run Notebook 02 first.\")\n",
    "        exit()\n",
    "\n",
    "    # 1. Create the store\n",
    "    vector_db = generate_vector_store(chunked_docs)\n",
    "    \n",
    "    # 2. Save the FAISS index for the Q&A notebook\n",
    "    save_index(vector_db, INDEX_SAVE_PATH)\n",
    "    \n",
    "    # 3. Test Retrieval\n",
    "    query = \"What did Demis say about the future of AI?\"\n",
    "    results = vector_db.similarity_search(query, k=2)\n",
    "    # ... rest of your print code\n",
    "    \n",
    "    print(\"\\nüîç SIMILARITY SEARCH TEST:\")\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"\\nResult {i+1} (Source: {res.metadata['source']}):\")\n",
    "        print(f\"{res.page_content[:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
