{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82478cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AgenticRAG\\LangChain-Transcript-QA\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting NeuralTranscript Chunking Pipeline ---\n",
      "‚úÇÔ∏è Initializing Recursive Splitting (Size: 1000, Overlap: 200)...\n",
      "‚úÖ Created 169 enriched chunks.\n",
      "\n",
      "--- CHUNK VALIDATION ---\n",
      "Metadata: {'source': 'Gfr50f6ZBvo', 'content_type': 'video_transcript', 'start_index': 0}\n",
      "Preview: the following is a conversation with demus hasabis ceo and co-founder of deepmind a company that has published and builds some of the most incredible ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: NeuralTranscript: Semantic Search & Q&A for YouTube Content\n",
    "MODULE: 02_SEMANTIC_CHUNKING\n",
    "-------------------------------------------------------------------------\n",
    "DESCRIPTION:\n",
    "This module transforms raw transcript text into semantically meaningful \n",
    "chunks. By adding source metadata to each chunk, we enable the RAG system \n",
    "to provide citations and structured context to the LLM (Gemini/Groq).\n",
    "\n",
    "AUTHOR: Engr. Inam Ullah Khan\n",
    "Master's Student in Data Science | Al-Farabi Kazakh National University\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "# NEW: Import from the dedicated text-splitters package\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "VIDEO_ID = \"Gfr50f6ZBvo\"\n",
    "INPUT_PATH = f\"data/transcripts/{VIDEO_ID}.txt\"\n",
    "\n",
    "# RAG Hyperparameters\n",
    "CHUNK_SIZE = 1000   \n",
    "CHUNK_OVERLAP = 200 \n",
    "\n",
    "# --- 2. CORE PROCESSING FUNCTIONS ---\n",
    "\n",
    "def load_processed_transcript(file_path: str) -> str:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Transcript not found at {file_path}. Run Notebook 01 first.\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_enriched_chunks(text: str, source_id: str) -> list[Document]:\n",
    "    print(f\"‚úÇÔ∏è Initializing Recursive Splitting (Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP})...\")\n",
    "    \n",
    "    # Updated Splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        add_start_index=True \n",
    "    )\n",
    "    \n",
    "    # Generate chunks as Document objects\n",
    "    # Note: Using create_documents is cleaner in the new API\n",
    "    enriched_docs = splitter.create_documents(\n",
    "        [text], \n",
    "        metadatas=[{\"source\": source_id, \"content_type\": \"video_transcript\"}]\n",
    "    )\n",
    "    \n",
    "    return enriched_docs\n",
    "\n",
    "# --- 3. EXECUTION PIPELINE ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- Starting NeuralTranscript Chunking Pipeline ---\")\n",
    "    \n",
    "    full_text = load_processed_transcript(INPUT_PATH)\n",
    "    chunked_docs = create_enriched_chunks(full_text, VIDEO_ID)\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(chunked_docs)} enriched chunks.\")\n",
    "    \n",
    "    # Preview\n",
    "    sample = chunked_docs[0]\n",
    "    print(f\"\\n--- CHUNK VALIDATION ---\\nMetadata: {sample.metadata}\\nPreview: {sample.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6489ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunks safely persisted to data/chunked_docs.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the chunks so Notebook 03 can use them\n",
    "with open(\"data/chunked_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunked_docs, f)\n",
    "\n",
    "print(\"‚úÖ Chunks safely persisted to data/chunked_docs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b5107",
   "metadata": {},
   "source": [
    "# üèÅ Summary: Step 02 | Semantic Chunking & Metadata Enrichment\n",
    "In this stage of the NeuralTranscript pipeline, we successfully transformed the raw, unstructured transcript into a structured dataset optimized for high-precision retrieval.\n",
    "\n",
    "# üß† Key Achievements\n",
    "- Recursive Splitting Logic: Instead of arbitrary cuts, we implemented a hierarchical approach using RecursiveCharacterTextSplitter. The system prioritizes splitting at paragraphs (\\n\\n), then sentences (.), ensuring that related ideas stay together within a single chunk.\n",
    "\n",
    "- Context Preservation: By applying a 200-character overlap, we created a \"sliding window\" effect. This ensures that the transition between chunks remains semantically fluid, preventing the loss of information that occurs when a sentence is sliced at the boundary.\n",
    "\n",
    "- Agentic Metadata Enrichment: Each text chunk was wrapped into a Document object and enriched with unique identifiers:\n",
    "\n",
    "-  source: The original YouTube Video ID for traceability.\n",
    "\n",
    "- chunk_id: Enables the future LLM to cite specific segments.\n",
    "\n",
    "- start_index: Provides the exact character position from the original transcript.\n",
    "\n",
    "# üìä Data Insights\n",
    "Input Size: ~133,000 characters (Demis Hassabis Interview).\n",
    "\n",
    "Output Yield: 169 semantically coherent chunks.\n",
    "\n",
    "Efficiency: The average chunk length of 1,000 characters is the \"sweet spot\" for modern embedding models like all-MiniLM-L6-v2, balancing information density with retrieval speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42255cee",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Evaluated multiple chunking strategies for long-form transcript data\n",
    "- Conducted both quantitative and qualitative analysis\n",
    "- Selected an optimal chunk configuration for embedding and retrieval\n",
    "\n",
    "**Next step:** Embedding generation and similarity-based retrieval  \n",
    "(`03_embedding_retrieval.ipynb`)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
