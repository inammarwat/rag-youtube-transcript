{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9e039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AgenticRAG\\LangChain-Transcript-QA\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# --- 1. CONFIGURATION ---\u001b[39;00m\n\u001b[32m     20\u001b[39m VIDEO_ID = \u001b[33m\"\u001b[39m\u001b[33mGfr50f6ZBvo\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.schema'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: NeuralTranscript: Semantic Search & Q&A for YouTube Content\n",
    "MODULE: 02_SEMANTIC_CHUNKING\n",
    "-------------------------------------------------------------------------\n",
    "DESCRIPTION:\n",
    "This module transforms raw transcript text into semantically meaningful \n",
    "chunks. By adding source metadata to each chunk, we enable the RAG system \n",
    "to provide citations and structured context to the LLM (Gemini/Groq).\n",
    "\n",
    "AUTHOR: Engr. Inam Ullah Khan\n",
    "Master's Student in Data Science | Al-Farabi Kazakh National University\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "VIDEO_ID = \"Gfr50f6ZBvo\"\n",
    "INPUT_PATH = f\"data/transcripts/{VIDEO_ID}.txt\"\n",
    "\n",
    "# RAG Hyperparameters (Based on your thesis research)\n",
    "CHUNK_SIZE = 1000   # Max characters per chunk\n",
    "CHUNK_OVERLAP = 200 # Context window between chunks to prevent data loss\n",
    "\n",
    "# --- 2. CORE PROCESSING FUNCTIONS ---\n",
    "\n",
    "def load_processed_transcript(file_path: str) -> str:\n",
    "    \"\"\"Loads the unified transcript text from Notebook 01.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"âŒ Transcript not found at {file_path}. Run Notebook 01 first.\")\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_enriched_chunks(text: str, source_id: str) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Splits text into chunks and attaches metadata for agentic retrieval.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The full transcript text.\n",
    "        source_id (str): The Video ID for citation purposes.\n",
    "        \n",
    "    Returns:\n",
    "        list[Document]: A list of LangChain Document objects with metadata.\n",
    "    \"\"\"\n",
    "    print(f\"âœ‚ï¸ Initializing Recursive Splitting (Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP})...\")\n",
    "    \n",
    "    # Initialize the splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"], # Hierarchy of split points\n",
    "        add_start_index=True # Tracks exactly where the chunk starts in the original text\n",
    "    )\n",
    "    \n",
    "    # Generate raw text chunks\n",
    "    raw_chunks = splitter.split_text(text)\n",
    "    \n",
    "    # Wrap in Document objects with enriched metadata\n",
    "    enriched_docs = [\n",
    "        Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"source\": source_id,\n",
    "                \"chunk_id\": i,\n",
    "                \"content_type\": \"video_transcript\",\n",
    "                \"char_length\": len(chunk)\n",
    "            }\n",
    "        )\n",
    "        for i, chunk in enumerate(raw_chunks)\n",
    "    ]\n",
    "    \n",
    "    return enriched_docs\n",
    "\n",
    "# --- 3. EXECUTION PIPELINE ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- Starting NeuralTranscript Chunking Pipeline ---\")\n",
    "    \n",
    "    # Step 1: Load Data\n",
    "    full_text = load_processed_transcript(INPUT_PATH)\n",
    "    \n",
    "    # Step 2: Semantic Chunking\n",
    "    chunked_docs = create_enriched_chunks(full_text, VIDEO_ID)\n",
    "    \n",
    "    # Step 3: Analysis & Validation\n",
    "    total_chunks = len(chunked_docs)\n",
    "    print(f\"âœ… Created {total_chunks} enriched chunks.\")\n",
    "    \n",
    "    # Verify the first chunk structure\n",
    "    sample = chunked_docs[0]\n",
    "    print(\"\\n--- CHUNK STRUCTURE VALIDATION ---\")\n",
    "    print(f\"Metadata: {sample.metadata}\")\n",
    "    print(f\"Preview: {sample.page_content[:150]}...\")\n",
    "\n",
    "    # For your Portfolio: Save a summary of chunks for visual confirmation\n",
    "    print(f\"\\nðŸš€ Ready for Notebook 03: Vector Embedding & FAISS Indexing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82478cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting NeuralTranscript Chunking Pipeline ---\n",
      "âœ‚ï¸ Initializing Recursive Splitting (Size: 1000, Overlap: 200)...\n",
      "âœ… Created 169 enriched chunks.\n",
      "\n",
      "--- CHUNK VALIDATION ---\n",
      "Metadata: {'source': 'Gfr50f6ZBvo', 'content_type': 'video_transcript', 'start_index': 0}\n",
      "Preview: the following is a conversation with demus hasabis ceo and co-founder of deepmind a company that has published and builds some of the most incredible ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: NeuralTranscript: Semantic Search & Q&A for YouTube Content\n",
    "MODULE: 02_SEMANTIC_CHUNKING\n",
    "-------------------------------------------------------------------------\n",
    "DESCRIPTION:\n",
    "This module transforms raw transcript text into semantically meaningful \n",
    "chunks. By adding source metadata to each chunk, we enable the RAG system \n",
    "to provide citations and structured context to the LLM (Gemini/Groq).\n",
    "\n",
    "AUTHOR: Engr. Inam Ullah Khan\n",
    "Master's Student in Data Science | Al-Farabi Kazakh National University\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "# NEW: Import from the dedicated text-splitters package\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "VIDEO_ID = \"Gfr50f6ZBvo\"\n",
    "INPUT_PATH = f\"data/transcripts/{VIDEO_ID}.txt\"\n",
    "\n",
    "# RAG Hyperparameters\n",
    "CHUNK_SIZE = 1000   \n",
    "CHUNK_OVERLAP = 200 \n",
    "\n",
    "# --- 2. CORE PROCESSING FUNCTIONS ---\n",
    "\n",
    "def load_processed_transcript(file_path: str) -> str:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"âŒ Transcript not found at {file_path}. Run Notebook 01 first.\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_enriched_chunks(text: str, source_id: str) -> list[Document]:\n",
    "    print(f\"âœ‚ï¸ Initializing Recursive Splitting (Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP})...\")\n",
    "    \n",
    "    # Updated Splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        add_start_index=True \n",
    "    )\n",
    "    \n",
    "    # Generate chunks as Document objects\n",
    "    # Note: Using create_documents is cleaner in the new API\n",
    "    enriched_docs = splitter.create_documents(\n",
    "        [text], \n",
    "        metadatas=[{\"source\": source_id, \"content_type\": \"video_transcript\"}]\n",
    "    )\n",
    "    \n",
    "    return enriched_docs\n",
    "\n",
    "# --- 3. EXECUTION PIPELINE ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- Starting NeuralTranscript Chunking Pipeline ---\")\n",
    "    \n",
    "    full_text = load_processed_transcript(INPUT_PATH)\n",
    "    chunked_docs = create_enriched_chunks(full_text, VIDEO_ID)\n",
    "    \n",
    "    print(f\"âœ… Created {len(chunked_docs)} enriched chunks.\")\n",
    "    \n",
    "    # Preview\n",
    "    sample = chunked_docs[0]\n",
    "    print(f\"\\n--- CHUNK VALIDATION ---\\nMetadata: {sample.metadata}\\nPreview: {sample.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0caf81f",
   "metadata": {},
   "source": [
    "# Text Chunking Analysis\n",
    "\n",
    "This notebook analyzes **text chunking strategies** for long-form unstructured text.\n",
    "The goal is to identify suitable chunk sizes and overlaps for downstream\n",
    "embedding and retrieval in a RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735542c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d74d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to transcript created in 01_data_ingestion.ipynb\n",
    "DATA_PATH = \"../data/transcripts\"\n",
    "\n",
    "# Replace with your video ID\n",
    "VIDEO_ID = \"VIDEO_ID_HERE\"\n",
    "\n",
    "file_path = os.path.join(DATA_PATH, f\"{VIDEO_ID}.txt\")\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Transcript loaded.\")\n",
    "print(f\"Total characters: {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf31535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw text into a LangChain Document\n",
    "documents = [Document(page_content=text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different chunking strategies to compare\n",
    "chunk_configs = [\n",
    "    {\"chunk_size\": 500, \"chunk_overlap\": 50},\n",
    "    {\"chunk_size\": 800, \"chunk_overlap\": 100},\n",
    "    {\"chunk_size\": 1000, \"chunk_overlap\": 150},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(\n",
    "    docs: List[Document],\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int\n",
    ") -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ff4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_results = {}\n",
    "\n",
    "for config in chunk_configs:\n",
    "    chunks = chunk_documents(\n",
    "        documents,\n",
    "        chunk_size=config[\"chunk_size\"],\n",
    "        chunk_overlap=config[\"chunk_overlap\"]\n",
    "    )\n",
    "    key = f'size_{config[\"chunk_size\"]}_overlap_{config[\"chunk_overlap\"]}'\n",
    "    chunked_results[key] = chunks\n",
    "\n",
    "    print(f\"\\nConfiguration: {key}\")\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607baae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect first chunk from each configuration\n",
    "for key, chunks in chunked_results.items():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Configuration: {key}\")\n",
    "    print(\"Chunk length:\", len(chunks[0].page_content))\n",
    "    print(\"Sample chunk preview:\\n\")\n",
    "    print(chunks[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3992a1f",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Smaller chunks (500 characters) produce more granular segments but risk\n",
    "  losing broader context.\n",
    "- Larger chunks (1000 characters) preserve context but reduce retrieval precision.\n",
    "- A chunk size of **~800 characters with overlap ~100** provides a good balance\n",
    "  between semantic completeness and retrievability.\n",
    "\n",
    "**Selected configuration for downstream RAG pipeline:**  \n",
    "`chunk_size = 800`, `chunk_overlap = 100`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa26a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save selected chunks for reuse\n",
    "SELECTED_KEY = \"size_800_overlap_100\"\n",
    "selected_chunks = chunked_results[SELECTED_KEY]\n",
    "\n",
    "os.makedirs(\"../data/chunks\", exist_ok=True)\n",
    "\n",
    "output_path = f\"../data/chunks/{VIDEO_ID}_chunks.txt\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, chunk in enumerate(selected_chunks):\n",
    "        f.write(f\"--- Chunk {i+1} ---\\n\")\n",
    "        f.write(chunk.page_content + \"\\n\\n\")\n",
    "\n",
    "print(f\"Chunks saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42255cee",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Evaluated multiple chunking strategies for long-form transcript data\n",
    "- Conducted both quantitative and qualitative analysis\n",
    "- Selected an optimal chunk configuration for embedding and retrieval\n",
    "\n",
    "**Next step:** Embedding generation and similarity-based retrieval  \n",
    "(`03_embedding_retrieval.ipynb`)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
