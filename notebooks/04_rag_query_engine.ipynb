{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d43983",
   "metadata": {},
   "source": [
    "# Notebook 04 ‚Äì RAG Query Engine\n",
    "\n",
    "## Objective\n",
    "\n",
    "The objective of this notebook is to implement the Retrieval-Augmented Generation (RAG) query engine.  \n",
    "This stage integrates semantic retrieval with a Large Language Model (LLM) to generate grounded, context-aware responses.\n",
    "\n",
    "---\n",
    "\n",
    "## Input\n",
    "\n",
    "- FAISS vector store containing embedded transcript chunks\n",
    "- User query\n",
    "- Google Gemini LLM\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "- Retrieved top-k relevant transcript chunks\n",
    "- Context-aware generated answer based only on retrieved content\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. Load the persisted FAISS vector index.\n",
    "2. Configure the retriever with top-k similarity search.\n",
    "3. Retrieve the most semantically relevant transcript chunks.\n",
    "4. Construct a structured prompt that injects retrieved context.\n",
    "5. Invoke the Gemini LLM with grounded context.\n",
    "6. Generate a final answer constrained to retrieved knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Step is Important\n",
    "\n",
    "Standard LLMs may hallucinate when answering domain-specific questions.\n",
    "By incorporating semantic retrieval:\n",
    "\n",
    "- The model is grounded in factual transcript data.\n",
    "- Hallucination risk is reduced.\n",
    "- Responses are context-aware and source-aligned.\n",
    "- The system becomes scalable to long-form content.\n",
    "\n",
    "This notebook completes the full end-to-end RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d1dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting NeuralTranscript Query Engine ---\n",
      "\n",
      "üìÇ Loading Vector Database...\n",
      "ü§ñ Initializing Google Gemini Pro...\n",
      "\n",
      "‚ùì User Query:\n",
      "What is the main topic of this video??\n",
      "\n",
      "‚è≥ Processing answer...\n",
      "\n",
      "‚ú® AI RESPONSE:\n",
      "\n",
      "The main topic is a conversation with Demas about the mission to solve intelligence and use it to address fundamental scientific and philosophical mysteries, including those related to physics, consciousness, life, and gravity, and to overcome impossible challenges.\n",
      "\n",
      "--- Query Completed Successfully ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: \n",
    "NeuralTranscript: A RAG-Based Semantic Search & Q&A System for YouTube Content\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "AUTHOR: Engr. Inam Ullah Khan\n",
    "Master's Student in Data Science | Al-Farabi Kazakh National University\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. CONFIGURATION & ENVIRONMENT SETUP\n",
    "# --------------------------------------------------\n",
    "\n",
    "load_dotenv()  # Make sure GOOGLE_API_KEY is inside your .env file\n",
    "INDEX_PATH = \"data/faiss_index\"\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. CORE FUNCTIONS\n",
    "# --------------------------------------------------\n",
    "\n",
    "def load_vector_store():\n",
    "    \"\"\"\n",
    "    Loads the FAISS index.\n",
    "    IMPORTANT: The embedding model must match the one used during indexing.\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading Vector Database...\")\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    vector_db = FAISS.load_local(\n",
    "        INDEX_PATH,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "\n",
    "    return vector_db\n",
    "\n",
    "\n",
    "def build_rag_chain(vector_db):\n",
    "    \"\"\"\n",
    "    Builds modern LangChain v1 RAG pipeline using LCEL.\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Initializing Google Gemini Pro...\")\n",
    "\n",
    "    # 1. Initialize Gemini LLM\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0.2,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # 2. Prompt Template (LCEL style)\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an AI Assistant specialized in analyzing video content.\n",
    "Use the following transcript context to answer the question.\n",
    "If the answer is not contained in the context, say you don't know.\n",
    "Keep the answer concise and professional.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\")\n",
    "\n",
    "    # 3. Create Retriever\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    # 4. Build RAG Chain using LCEL\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. EXECUTION PIPELINE\n",
    "# --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"\\n--- Starting NeuralTranscript Query Engine ---\\n\")\n",
    "\n",
    "    # Step 1: Load Vector Database\n",
    "    db = load_vector_store()\n",
    "\n",
    "    # Step 2: Build RAG Chain\n",
    "    neural_qa = build_rag_chain(db)\n",
    "\n",
    "    # Step 3: User Query\n",
    "    user_query = \"What is the main topic of this video??\"\n",
    "\n",
    "    print(f\"\\n‚ùì User Query:\\n{user_query}\")\n",
    "    print(\"\\n‚è≥ Processing answer...\\n\")\n",
    "\n",
    "    # Step 4: Invoke Chain\n",
    "    response = neural_qa.invoke(user_query)\n",
    "\n",
    "    # Step 5: Display Result\n",
    "    print(\"‚ú® AI RESPONSE:\\n\")\n",
    "    print(response)\n",
    "    print(\"\\n--- Query Completed Successfully ---\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b3867",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Retriever returned top 3 semantically relevant chunks.\n",
    "- Retrieved context contained direct information related to the query.\n",
    "- Gemini generated an answer grounded in the retrieved content.\n",
    "- No hallucinated information observed in test query.\n",
    "- Response quality improved compared to direct LLM call without retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda8c12",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Retrieval-Augmented Generation pipeline was successfully implemented.\n",
    "The retriever dynamically selects semantically relevant transcript segments, which are injected into the prompt for grounded response generation.\n",
    "This architecture improves factual reliability and reduces hallucination risk compared to standalone LLM usage.\n",
    "The system demonstrates a functional end-to-end RAG workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da841fe",
   "metadata": {},
   "source": [
    "### üöÄ Project Conclusion\n",
    "\n",
    "The **NeuralTranscript** project is now a fully functional end-to-end RAG system. It demonstrates proficiency in **Data Engineering** (Ingestion/Chunking), **Vector Mathematics** (Indexing), and **Generative AI** (RAG Orchestration).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
