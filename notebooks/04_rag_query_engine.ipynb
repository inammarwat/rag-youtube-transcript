{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d1dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting NeuralTranscript Query Engine ---\n",
      "\n",
      "üìÇ Loading Vector Database...\n",
      "ü§ñ Initializing Google Gemini Pro...\n",
      "\n",
      "‚ùì User Query:\n",
      "What is the main topic of this video??\n",
      "\n",
      "‚è≥ Processing answer...\n",
      "\n",
      "‚ú® AI RESPONSE:\n",
      "\n",
      "The main topic of this video is a conversation with Demas about solving fundamental mysteries of the universe, including consciousness, life, and gravity, and the search for deeper explanations beyond the standard model of physics, potentially through the application of intelligence and reinforcement learning.\n",
      "\n",
      "--- Query Completed Successfully ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: NeuralTranscript: Semantic Search & Q&A for YouTube Content\n",
    "MODULE: 04_RAG_QUERY_ENGINE\n",
    "-------------------------------------------------------------------------\n",
    "DESCRIPTION:\n",
    "Final stage of the pipeline. It takes a user query, retrieves\n",
    "relevant context from the FAISS vector store, and uses Google Gemini to \n",
    "generate a precise, context-aware answer based on the video transcript.\n",
    "\n",
    "AUTHOR: Engr. Inam Ullah Khan\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. CONFIGURATION & ENVIRONMENT SETUP\n",
    "# --------------------------------------------------\n",
    "\n",
    "load_dotenv()  # Make sure GOOGLE_API_KEY is inside your .env file\n",
    "INDEX_PATH = \"data/faiss_index\"\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. CORE FUNCTIONS\n",
    "# --------------------------------------------------\n",
    "\n",
    "def load_vector_store():\n",
    "    \"\"\"\n",
    "    Loads the FAISS index.\n",
    "    IMPORTANT: The embedding model must match the one used during indexing.\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading Vector Database...\")\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    vector_db = FAISS.load_local(\n",
    "        INDEX_PATH,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "\n",
    "    return vector_db\n",
    "\n",
    "\n",
    "def build_rag_chain(vector_db):\n",
    "    \"\"\"\n",
    "    Builds modern LangChain v1 RAG pipeline using LCEL.\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Initializing Google Gemini Pro...\")\n",
    "\n",
    "    # 1. Initialize Gemini LLM\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0.2,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # 2. Prompt Template (LCEL style)\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an AI Assistant specialized in analyzing video content.\n",
    "Use the following transcript context to answer the question.\n",
    "If the answer is not contained in the context, say you don't know.\n",
    "Keep the answer concise and professional.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\")\n",
    "\n",
    "    # 3. Create Retriever\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    # 4. Build RAG Chain using LCEL\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. EXECUTION PIPELINE\n",
    "# --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"\\n--- Starting NeuralTranscript Query Engine ---\\n\")\n",
    "\n",
    "    # Step 1: Load Vector Database\n",
    "    db = load_vector_store()\n",
    "\n",
    "    # Step 2: Build RAG Chain\n",
    "    neural_qa = build_rag_chain(db)\n",
    "\n",
    "    # Step 3: User Query\n",
    "    user_query = \"What is the main topic of this video??\"\n",
    "\n",
    "    print(f\"\\n‚ùì User Query:\\n{user_query}\")\n",
    "    print(\"\\n‚è≥ Processing answer...\\n\")\n",
    "\n",
    "    # Step 4: Invoke Chain\n",
    "    response = neural_qa.invoke(user_query)\n",
    "\n",
    "    # Step 5: Display Result\n",
    "    print(\"‚ú® AI RESPONSE:\\n\")\n",
    "    print(response)\n",
    "    print(\"\\n--- Query Completed Successfully ---\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da841fe",
   "metadata": {},
   "source": [
    "## üìä Observations & Technical Analysis\n",
    "\n",
    "* **System Synergy (RAG Validation)**: The execution demonstrates a successful integration of the **FAISS** vector store with the **Gemini 2.5 Flash** model. The \"I don't know\" logic in the prompt prevents hallucinations, ensuring the AI remains grounded in the provided transcript context.\n",
    "* **LCEL Pipeline Efficiency**: The use of **LangChain Expression Language (LCEL)** provides a transparent and efficient data flow. By using `RunnablePassthrough` and `StrOutputParser`, the system minimizes latency between context retrieval and answer generation.\n",
    "* **Prompt Precision**: The human-centered prompt template successfully guides the LLM to maintain a professional and concise tone. This is critical for applications where the user requires factual summaries rather than creative interpretations.\n",
    "* **Contextual Retrieval Performance**: With `k=3`, the retriever provides approximately 3,000 characters of context (based on our 1,000-character chunk size). This fills the LLM's context window with high-density information, allowing the model to \"reason\" across different segments of the video.\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ Summary: Module 04 ‚Äî RAG Query Engine\n",
    "\n",
    "This module represents the completion of the **NeuralTranscript** pipeline. We have successfully transformed a raw YouTube transcript into an intelligent, queryable research tool.\n",
    "\n",
    "### üõ†Ô∏è Key Technical Deliverables:\n",
    "\n",
    "1. **Neural Retrieval Integration**: Connected the local FAISS index to the LangChain retrieval chain, allowing for semantic-based context fetching.\n",
    "2. **Gemini 2.5 Implementation**: Leveraged Google's latest generative model to interpret retrieved video segments and synthesize natural language answers.\n",
    "3. **Modern Chain Architecture**: Developed the engine using **LCEL**, moving away from legacy chains to a more modular, future-proof codebase.\n",
    "4. **Zero-Shot Reliability**: Implemented strict grounding instructions in the prompt to ensure the system only answers based on the transcript, maintaining high factual integrity.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Project Conclusion\n",
    "\n",
    "The **NeuralTranscript** project is now a fully functional end-to-end RAG system. It demonstrates proficiency in **Data Engineering** (Ingestion/Chunking), **Vector Mathematics** (Indexing), and **Generative AI** (RAG Orchestration).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
