{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4171cae",
   "metadata": {},
   "source": [
    "# Notebook 03 ‚Äì Embedding & Vector Indexing\n",
    "\n",
    "## Objective\n",
    "\n",
    "The objective of this notebook is to convert preprocessed transcript chunks into vector embeddings and construct a searchable vector index for semantic retrieval.  \n",
    "This stage forms the core knowledge base of the Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## Input\n",
    "\n",
    "- List of cleaned and chunked transcript segments (from Notebook 02)\n",
    "- Embedding model (Google Gemini or compatible embedding provider)\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "- Vector embeddings for all transcript chunks\n",
    "- FAISS vector store containing indexed embeddings\n",
    "- Retriever object capable of performing semantic similarity search\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. Load chunked transcript data.\n",
    "2. Initialize embedding model.\n",
    "3. Generate embeddings for each chunk.\n",
    "4. Build a FAISS vector index.\n",
    "5. Configure retriever with top-k similarity search.\n",
    "6. Test retrieval using a sample query.\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Step is Important\n",
    "\n",
    "Large Language Models (LLMs) cannot efficiently process long documents directly due to context window limitations.  \n",
    "By converting text chunks into dense vector representations:\n",
    "\n",
    "- Semantic similarity search becomes possible.\n",
    "- Relevant context can be retrieved dynamically.\n",
    "- Hallucination risk is reduced.\n",
    "- Response quality improves through grounded context injection.\n",
    "\n",
    "This notebook completes the **knowledge indexing stage** of the RAG architecture and prepares the system for generative response integration in Notebook 04.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ca0fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting NeuralTranscript Indexing Pipeline ---\n",
      "üì• Successfully loaded 169 chunks from disk.\n",
      "üß† Initializing Neural Embedding Model (HuggingFace)...\n",
      "üöÄ Generating embeddings for 169 chunks. Please wait...\n",
      "üíæ FAISS Index successfully saved to: data/faiss_index\n",
      "\n",
      "üîç SIMILARITY SEARCH TEST:\n",
      "\n",
      "Result 1 (Source: Gfr50f6ZBvo):\n",
      "from a sentient animal and we know they're made of the same things biological neurons so we're gonna have to come up with explanations uh or models of the gap between substrate differences between mac...\n",
      "\n",
      "Result 2 (Source: Gfr50f6ZBvo):\n",
      "part of of birthing ai and that being the greatest benefit to humanity of any tool or technology ever and and getting us into a world of radical abundance and curing diseases and and and solving many ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: \n",
    "NeuralTranscript: A RAG-Based Semantic Search & Q&A System for YouTube Content\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "AUTHOR: Engr. Inam Ullah Khan\n",
    "Master's Student in Data Science | Al-Farabi Kazakh National University\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# Note: Use your chunked_docs from the previous step. \n",
    "# In a real pipeline, you might reload them or run them in the same session.\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress standard Python warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "# Suppress the Xet Storage/HTTP fallback message\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "INDEX_SAVE_PATH = \"data/faiss_index\"\n",
    "\n",
    "# --- 2. CORE FUNCTIONS ---\n",
    "\n",
    "def generate_vector_store(documents):\n",
    "    \"\"\"\n",
    "    Converts documents to embeddings and stores them in a FAISS index.\n",
    "    \"\"\"\n",
    "    print(\"üß† Initializing Neural Embedding Model (HuggingFace)...\")\n",
    "    \n",
    "    # Using a high-quality, lightweight model included in your requirements\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'} # Use 'cuda' if you have a GPU in Colab\n",
    "    )\n",
    "    \n",
    "    print(f\"üöÄ Generating embeddings for {len(documents)} chunks. Please wait...\")\n",
    "    \n",
    "    # Create the FAISS index from the documents\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def save_index(vector_store, path):\n",
    "    \"\"\"Persists the FAISS index to the local disk.\"\"\"\n",
    "    vector_store.save_local(path)\n",
    "    print(f\"üíæ FAISS Index successfully saved to: {path}\")\n",
    "\n",
    "# --- 3. EXECUTION PIPELINE (UPDATED) ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting NeuralTranscript Indexing Pipeline ---\")\n",
    "    \n",
    "    # NEW: Load the chunks from the disk\n",
    "    try:\n",
    "        with open(\"data/chunked_docs.pkl\", \"rb\") as f:\n",
    "            chunked_docs = pickle.load(f)\n",
    "        print(f\"üì• Successfully loaded {len(chunked_docs)} chunks from disk.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: chunked_docs.pkl not found. Please run Notebook 02 first.\")\n",
    "        exit()\n",
    "\n",
    "    # 1. Create the store\n",
    "    vector_db = generate_vector_store(chunked_docs)\n",
    "    \n",
    "    # 2. Save the FAISS index for the Q&A notebook\n",
    "    save_index(vector_db, INDEX_SAVE_PATH)\n",
    "    \n",
    "    # 3. Test Retrieval\n",
    "    query = \"What did Demis say about the future of AI?\"\n",
    "    results = vector_db.similarity_search(query, k=2)\n",
    "    # ... rest of your print code\n",
    "    \n",
    "    print(\"\\nüîç SIMILARITY SEARCH TEST:\")\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"\\nResult {i+1} (Source: {res.metadata['source']}):\")\n",
    "        print(f\"{res.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c2bd4",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Successfully generated embeddings for 169 chunks.\n",
    "- Embedding dimensionality: 384.\n",
    "- FAISS index constructed without dimensional mismatch.\n",
    "- Retrieval test query returned top 3 semantically relevant chunks.\n",
    "- Retrieved content aligns with expected video topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d65659",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The transcript chunks were successfully converted into vector embeddings and indexed using FAISS.\n",
    "Similarity-based retrieval returns semantically relevant context segments, validating the effectiveness of the embedding model.\n",
    "This completes the knowledge indexing stage of the RAG architecture and prepares the system for generative response integration.\n",
    "\n",
    "\n",
    "**Next step:** RAG Query Engine  \n",
    "(`04_rag_query_engine.ipynb`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
