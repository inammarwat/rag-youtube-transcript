{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff7831a",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation Pipeline (Gemini)\n",
    "\n",
    "This notebook implements the **final RAG pipeline** by integrating:\n",
    "- Semantic retrieval from a FAISS vector store\n",
    "- Answer generation using Google Gemini Pro\n",
    "\n",
    "The system produces **grounded answers** along with retrieved source chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853355a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b557bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "print(\"Environment variables loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f948e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54552d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DB_PATH = \"../vectorstore/faiss_index\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    VECTOR_DB_PATH,\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\"FAISS vector store loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b35881",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"Gemini LLM initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "print(\"Retriever configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"RAG pipeline constructed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic discussed in this video?\"\n",
    "\n",
    "response = qa_chain(query)\n",
    "\n",
    "print(\"Answer:\\n\")\n",
    "print(response[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRetrieved source chunks:\\n\")\n",
    "\n",
    "for i, doc in enumerate(response[\"source_documents\"], start=1):\n",
    "    print(f\"--- Source {i} ---\")\n",
    "    print(doc.page_content[:400])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821713c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_query = \"Explain the key idea in simple terms.\"\n",
    "\n",
    "follow_up_response = qa_chain(follow_up_query)\n",
    "\n",
    "print(\"Follow-up Answer:\\n\")\n",
    "print(follow_up_response[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140572cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RAG pipeline executed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970a76c",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- The RAG system generates coherent and context-aware answers.\n",
    "- Retrieved chunks align well with the user query, validating the embedding and chunking strategy.\n",
    "- Gemini Pro produces concise and grounded responses when provided with retrieved context.\n",
    "\n",
    "This confirms the effectiveness of the end-to-end RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a39884d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Loaded a persisted FAISS vector store\n",
    "- Retrieved semantically relevant transcript chunks\n",
    "- Generated grounded answers using Gemini Pro\n",
    "- Returned source documents for transparency\n",
    "\n",
    "The RAG pipeline is now **complete and reusable**.\n",
    "\n",
    "This notebook serves as the **final experimental validation** before\n",
    "consolidating the system into `main.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dfc4ba9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrieval_qa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# --- 1. CONFIGURATION & ENV SETUP ---\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: NeuralTranscript: Semantic Search & Q&A for YouTube Content\n",
    "MODULE: 04_RAG_QUERY_ENGINE\n",
    "-------------------------------------------------------------------------\n",
    "DESCRIPTION:\n",
    "This is the final stage of the pipeline. It takes a user query, retrieves\n",
    "relevant context from the FAISS vector store, and uses Google Gemini to \n",
    "generate a precise, context-aware answer based on the video transcript.\n",
    "\n",
    "AUTHOR: Engr. Inam Ullah Khan\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# --- 1. CONFIGURATION & ENV SETUP ---\n",
    "load_dotenv() # Ensure your GOOGLE_API_KEY is in your .env file\n",
    "INDEX_PATH = \"data/faiss_index\"\n",
    "\n",
    "# --- 2. CORE FUNCTIONS ---\n",
    "\n",
    "def load_vector_store():\n",
    "    \"\"\"\n",
    "    Loads the FAISS index. Note: We must provide the same embedding \n",
    "    function used during indexing to 'understand' the vectors.\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading Vector Database...\")\n",
    "    \n",
    "    # We use the same model as Notebook 03 for consistency\n",
    "    # (If you used HuggingFace in 03, we use it here to load)\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    vector_db = FAISS.load_local(\n",
    "        INDEX_PATH, \n",
    "        embeddings, \n",
    "        allow_dangerous_deserialization=True # Required for loading local pkl files\n",
    "    )\n",
    "    return vector_db\n",
    "\n",
    "def build_rag_chain(vector_db):\n",
    "    \"\"\"\n",
    "    Sets up the RAG pipeline: Retriever + Prompt + Gemini LLM.\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Initializing Google Gemini Pro...\")\n",
    "    \n",
    "    # 1. Initialize Gemini\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=0.2, # Lower temperature for factual accuracy\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # 2. Create a Custom Prompt (Human-Centered Design)\n",
    "    template = \"\"\"\n",
    "    You are an AI Assistant specialized in analyzing video content. \n",
    "    Use the following pieces of retrieved context from a video transcript to answer the question. \n",
    "    If you don't know the answer based on the context, just say you don't know. \n",
    "    Keep the answer concise and professional.\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    QUESTION: \n",
    "    {question}\n",
    "\n",
    "    HELPFUL ANSWER:\n",
    "    \"\"\"\n",
    "    \n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    # 3. Create the Chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\", # \"Stuffs\" all retrieved context into the prompt\n",
    "        retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True # Critical for citations\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "# --- 3. EXECUTION PIPELINE ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting NeuralTranscript Query Engine ---\")\n",
    "    \n",
    "    # Step 1: Load the \"Memory\"\n",
    "    db = load_vector_store()\n",
    "    \n",
    "    # Step 2: Initialize the \"Brain\"\n",
    "    neural_qa = build_rag_chain(db)\n",
    "    \n",
    "    # Step 3: Interactive Query\n",
    "    user_query = \"What is Demis Hassabis's view on the potential of AI to solve scientific problems?\"\n",
    "    \n",
    "    print(f\"\\n‚ùì User Query: {user_query}\")\n",
    "    print(\"‚è≥ Processing answer...\\n\")\n",
    "    \n",
    "    response = neural_qa.invoke({\"query\": user_query})\n",
    "    \n",
    "    # Step 4: Display Result\n",
    "    print(\"‚ú® AI RESPONSE:\")\n",
    "    print(response[\"result\"])\n",
    "    \n",
    "    print(\"\\nüìö SOURCES USED:\")\n",
    "    for doc in response[\"source_documents\"]:\n",
    "        print(f\"- Chunk ID: {doc.metadata.get('chunk_id')} (Source: {doc.metadata.get('source')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceaab3dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine_documents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_retrieval_chain\n\u001b[32m     13\u001b[39m load_dotenv()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: NeuralTranscript: Semantic Search & Q&A for YouTube Content\n",
    "MODULE: 04_RAG_QUERY_ENGINE (v2026 Fixed)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def build_rag_chain(vector_db):\n",
    "    print(\"ü§ñ Initializing Gemini 1.5 Pro & Modern RAG Chain...\")\n",
    "    \n",
    "    # 1. Initialize Gemini\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.2)\n",
    "    \n",
    "    # 2. Define the System Prompt (Human-Centered Instruction)\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer the question. \"\n",
    "        \"If you don't know the answer, say that you don't know. \"\n",
    "        \"Use three sentences maximum and keep the answer concise.\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    # 3. Create the \"Stuff Documents\" Chain (Handles the context)\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    # 4. Create the final Retrieval Chain\n",
    "    rag_chain = create_retrieval_chain(\n",
    "        vector_db.as_retriever(search_kwargs={\"k\": 3}), \n",
    "        question_answer_chain\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # (Assuming load_vector_store() from previous response is above)\n",
    "    db = load_vector_store()\n",
    "    neural_qa = build_rag_chain(db)\n",
    "    \n",
    "    query = \"What is Demis Hassabis's view on the potential of AI to solve scientific problems?\"\n",
    "    \n",
    "    # In modern LangChain, we use 'input' instead of 'query'\n",
    "    response = neural_qa.invoke({\"input\": query})\n",
    "    \n",
    "    print(\"\\n‚ú® AI RESPONSE:\")\n",
    "    print(response[\"answer\"]) # Note: Key is 'answer' in modern chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14843a86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# --- EXECUTION ---\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     db = \u001b[43mload_vector_store\u001b[49m()\n\u001b[32m     40\u001b[39m     neural_qa = build_rag_chain(db)\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# In LCEL, we simply pass the string question\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'load_vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: NeuralTranscript: Semantic Search & Q&A for YouTube Content\n",
    "MODULE: 04_RAG_QUERY_ENGINE (Modern LCEL Version)\n",
    "\"\"\"\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def build_rag_chain(vector_db):\n",
    "    print(\"ü§ñ Initializing Modern LCEL RAG Chain with Gemini...\")\n",
    "    \n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.2)\n",
    "    \n",
    "    # 1. Define the Human-Centered Prompt\n",
    "    template = \"\"\"You are a research assistant for the NeuralTranscript project. \n",
    "    Use the transcript excerpts below to answer the user's question.\n",
    "    \n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    # 2. Define the LCEL Chain Logic\n",
    "    # This replaces the 'create_retrieval_chain' with a transparent flow\n",
    "    rag_chain = (\n",
    "        {\"context\": vector_db.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    db = load_vector_store()\n",
    "    neural_qa = build_rag_chain(db)\n",
    "    \n",
    "    # In LCEL, we simply pass the string question\n",
    "    response = neural_qa.invoke(\"How does Demis view AI's role in science?\")\n",
    "    print(f\"\\n‚ú® AI RESPONSE:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d1dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting NeuralTranscript Query Engine ---\n",
      "\n",
      "üìÇ Loading Vector Database...\n",
      "ü§ñ Initializing Google Gemini Pro...\n",
      "\n",
      "‚ùì User Query:\n",
      "What is the main topic of this video??\n",
      "\n",
      "‚è≥ Processing answer...\n",
      "\n",
      "‚ú® AI RESPONSE:\n",
      "\n",
      "The main topic of this video is a conversation with Demas about solving fundamental mysteries of the universe, including consciousness, life, and gravity, and the search for deeper explanations beyond the standard model of physics, potentially through the application of intelligence and reinforcement learning.\n",
      "\n",
      "--- Query Completed Successfully ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROJECT: NeuralTranscript: Semantic Search & Q&A for YouTube Content\n",
    "MODULE: 04_RAG_QUERY_ENGINE\n",
    "-------------------------------------------------------------------------\n",
    "DESCRIPTION:\n",
    "Final stage of the pipeline. It takes a user query, retrieves\n",
    "relevant context from the FAISS vector store, and uses Google Gemini to \n",
    "generate a precise, context-aware answer based on the video transcript.\n",
    "\n",
    "AUTHOR: Engr. Inam Ullah Khan\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. CONFIGURATION & ENVIRONMENT SETUP\n",
    "# --------------------------------------------------\n",
    "\n",
    "load_dotenv()  # Make sure GOOGLE_API_KEY is inside your .env file\n",
    "INDEX_PATH = \"data/faiss_index\"\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. CORE FUNCTIONS\n",
    "# --------------------------------------------------\n",
    "\n",
    "def load_vector_store():\n",
    "    \"\"\"\n",
    "    Loads the FAISS index.\n",
    "    IMPORTANT: The embedding model must match the one used during indexing.\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading Vector Database...\")\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    vector_db = FAISS.load_local(\n",
    "        INDEX_PATH,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "\n",
    "    return vector_db\n",
    "\n",
    "\n",
    "def build_rag_chain(vector_db):\n",
    "    \"\"\"\n",
    "    Builds modern LangChain v1 RAG pipeline using LCEL.\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Initializing Google Gemini Pro...\")\n",
    "\n",
    "    # 1. Initialize Gemini LLM\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0.2,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # 2. Prompt Template (LCEL style)\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an AI Assistant specialized in analyzing video content.\n",
    "Use the following transcript context to answer the question.\n",
    "If the answer is not contained in the context, say you don't know.\n",
    "Keep the answer concise and professional.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\")\n",
    "\n",
    "    # 3. Create Retriever\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    # 4. Build RAG Chain using LCEL\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. EXECUTION PIPELINE\n",
    "# --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"\\n--- Starting NeuralTranscript Query Engine ---\\n\")\n",
    "\n",
    "    # Step 1: Load Vector Database\n",
    "    db = load_vector_store()\n",
    "\n",
    "    # Step 2: Build RAG Chain\n",
    "    neural_qa = build_rag_chain(db)\n",
    "\n",
    "    # Step 3: User Query\n",
    "    user_query = \"What is the main topic of this video??\"\n",
    "\n",
    "    print(f\"\\n‚ùì User Query:\\n{user_query}\")\n",
    "    print(\"\\n‚è≥ Processing answer...\\n\")\n",
    "\n",
    "    # Step 4: Invoke Chain\n",
    "    response = neural_qa.invoke(user_query)\n",
    "\n",
    "    # Step 5: Display Result\n",
    "    print(\"‚ú® AI RESPONSE:\\n\")\n",
    "    print(response)\n",
    "    print(\"\\n--- Query Completed Successfully ---\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
