{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff7831a",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation Pipeline (Gemini)\n",
    "\n",
    "This notebook implements the **final RAG pipeline** by integrating:\n",
    "- Semantic retrieval from a FAISS vector store\n",
    "- Answer generation using Google Gemini Pro\n",
    "\n",
    "The system produces **grounded answers** along with retrieved source chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853355a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b557bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "print(\"Environment variables loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f948e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54552d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DB_PATH = \"../vectorstore/faiss_index\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    VECTOR_DB_PATH,\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\"FAISS vector store loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b35881",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"Gemini LLM initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "print(\"Retriever configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"RAG pipeline constructed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic discussed in this video?\"\n",
    "\n",
    "response = qa_chain(query)\n",
    "\n",
    "print(\"Answer:\\n\")\n",
    "print(response[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRetrieved source chunks:\\n\")\n",
    "\n",
    "for i, doc in enumerate(response[\"source_documents\"], start=1):\n",
    "    print(f\"--- Source {i} ---\")\n",
    "    print(doc.page_content[:400])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821713c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_query = \"Explain the key idea in simple terms.\"\n",
    "\n",
    "follow_up_response = qa_chain(follow_up_query)\n",
    "\n",
    "print(\"Follow-up Answer:\\n\")\n",
    "print(follow_up_response[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140572cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RAG pipeline executed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970a76c",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- The RAG system generates coherent and context-aware answers.\n",
    "- Retrieved chunks align well with the user query, validating the embedding and chunking strategy.\n",
    "- Gemini Pro produces concise and grounded responses when provided with retrieved context.\n",
    "\n",
    "This confirms the effectiveness of the end-to-end RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a39884d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Loaded a persisted FAISS vector store\n",
    "- Retrieved semantically relevant transcript chunks\n",
    "- Generated grounded answers using Gemini Pro\n",
    "- Returned source documents for transparency\n",
    "\n",
    "The RAG pipeline is now **complete and reusable**.\n",
    "\n",
    "This notebook serves as the **final experimental validation** before\n",
    "consolidating the system into `main.py`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
